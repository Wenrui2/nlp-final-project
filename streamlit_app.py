import streamlit as st
import json
import time
import PyPDF2  # æ–°å¢ï¼šç”¨äºè§£æPDF
import io      # æ–°å¢ï¼šç”¨äºå¤„ç†å­—èŠ‚æµ
from langchain_community.chat_models import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

# --- 1. ç³»ç»Ÿé…ç½®ä¸å…¨å±€è®¾ç½® (ç³»ç»Ÿè®¾è®¡ï¼šå‰ç«¯å±‚) ---
st.set_page_config(
    page_title="DeepSeek NLP æ™ºèƒ½åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– Session State (å…³é”®æŠ€æœ¯ç‚¹ï¼šçŠ¶æ€ç®¡ç†)
# ç”¨äºå­˜å‚¨èŠå¤©è®°å½•ï¼Œå®ç°å¤šè½®å¯¹è¯
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False

# --- 2. ä¾§è¾¹æ é…ç½®åŒº (ç³»ç»Ÿè®¾è®¡ï¼šæ§åˆ¶å±‚) ---
with st.sidebar:
    st.title("ğŸ› ï¸ ç³»ç»Ÿæ§åˆ¶å°")
    st.markdown("---")
    
    # 2.1 API é…ç½®
    st.subheader("1. æ¥å£è®¾ç½®")
    openai_api_key = st.text_input('API Key (å¯†é’¥)', type='password', help="è¯·è¾“å…¥ SiliconFlow/DeepSeek çš„ API Key")
    
    # 2.2 æ¨¡å‹å‚æ•° (ä½“ç°å¯¹NLPå‚æ•°çš„ç†è§£ - è¯¦ç»†è®¾è®¡ç‚¹)
    st.subheader("2. æ¨¡å‹å‚æ•°")
    temperature = st.slider("åˆ›æ–°åº¦ (Temperature)", 0.0, 1.5, 0.7, 0.1, help="å€¼è¶Šé«˜å›å¤è¶Šå‘æ•£ï¼Œå€¼è¶Šä½è¶Šä¸¥è°¨")
    max_tokens = st.number_input("æœ€å¤§é•¿åº¦ (Max Tokens)", 512, 4096, 2048)
    
    # 2.3 è§’è‰²è®¾å®š (åˆ›æ–°ç‚¹ï¼šPrompt Engineering)
    st.subheader("3. è§’è‰²è®¾å®š")
    system_role = st.selectbox(
        "é€‰æ‹© AI æ‰®æ¼”çš„è§’è‰²",
        ["é€šç”¨æ™ºèƒ½åŠ©æ‰‹", "NLP å­¦æœ¯ä¸“å®¶", "Python ä»£ç å®¡è®¡å‘˜", "è‹æ ¼æ‹‰åº•å¼å¯¼å¸ˆ"],
        index=0
    )
    
    # 2.4 æ•°æ®ç®¡ç†
    st.markdown("---")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()
        
    # å¯¼å‡ºåŠŸèƒ½ (åŠ åˆ†é¡¹ï¼šåŠŸèƒ½å®Œæ•´æ€§)
    if st.session_state.messages:
        chat_str = json.dumps([{"role": m["role"], "content": m["content"]} for m in st.session_state.messages], ensure_ascii=False, indent=2)
        st.download_button(
            label="ğŸ“¥ å¯¼å‡ºèŠå¤©è®°å½• (JSON)",
            data=chat_str,
            file_name="chat_history.json",
            mime="application/json"
        )

# --- 3. æ ¸å¿ƒé€»è¾‘å‡½æ•° (ç³»ç»Ÿè®¾è®¡ï¼šé€»è¾‘å±‚) ---

def get_system_prompt(role):
    """æ ¹æ®é€‰æ‹©çš„è§’è‰²è¿”å› System Prompt"""
    prompts = {
        "é€šç”¨æ™ºèƒ½åŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚",
        "NLP å­¦æœ¯ä¸“å®¶": "ä½ æ˜¯ä¸€åè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„èµ„æ·±æ•™æˆã€‚è¯·ç”¨å­¦æœ¯ã€ä¸¥è°¨çš„å£å»å›ç­”ï¼Œå¹¶å¤šå¼•ç”¨BERT, Transformer, LLMç­‰æŠ€æœ¯åŸç†ã€‚",
        "Python ä»£ç å®¡è®¡å‘˜": "ä½ æ˜¯ä¸€åèµ„æ·±ç¨‹åºå‘˜ã€‚è¯·æ£€æŸ¥ç”¨æˆ·çš„ä»£ç ï¼ŒæŒ‡å‡ºæ½œåœ¨Bugï¼Œå¹¶ç»™å‡ºä¼˜åŒ–åçš„ä»£ç ã€‚",
        "è‹æ ¼æ‹‰åº•å¼å¯¼å¸ˆ": "ä½ æ˜¯ä¸€åå¯¼å¸ˆã€‚ä¸è¦ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œè€Œæ˜¯é€šè¿‡æé—®å¼•å¯¼ç”¨æˆ·è‡ªå·±æ€è€ƒå¾—å‡ºç»“è®ºã€‚"
    }
    return prompts.get(role, "You are a helpful assistant.")

def call_llm(messages_payload):
    """å°è£… LLM è°ƒç”¨é€»è¾‘ï¼ŒåŒ…å«é”™è¯¯å¤„ç†"""
    if not openai_api_key:
        st.error("ğŸš« è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ è¾“å…¥ API Key")
        return None
        
    llm = ChatOpenAI(
        temperature=temperature,
        openai_api_key=openai_api_key,
        base_url="https://api.siliconflow.cn/v1", # ç¡…åŸºæµåŠ¨åœ°å€
        model_name="deepseek-ai/DeepSeek-V3",     # æ¨¡å‹åç§°
        max_tokens=max_tokens
    )
    
    try:
        response = llm.invoke(messages_payload)
        return response.content
    except Exception as e:
        st.error(f"âŒ API è°ƒç”¨å¤±è´¥: {str(e)}")
        return None
        
# --- 3.1 æ–°å¢ï¼šæ–‡æ¡£å¤„ç†å‡½æ•° (NLP éç»“æ„åŒ–æ•°æ®å¤„ç†) ---
def extract_text_from_file(uploaded_file):
    """ä»ä¸Šä¼ çš„æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
    content = ""
    try:
        if uploaded_file.type == "application/pdf":
            # å¤„ç† PDF
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                content += page.extract_text() or ""
        elif uploaded_file.type == "text/plain":
            # å¤„ç† TXT
            content = uploaded_file.getvalue().decode("utf-8")
        return content
    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶å¤±è´¥: {e}")
        return None

# --- 4. ä¸»ç•Œé¢å¸ƒå±€ (ç³»ç»Ÿè®¾è®¡ï¼šè§†å›¾å±‚) ---
st.title('ğŸ§  NLP æœŸæœ«å¤§ä½œä¸š - æ™ºèƒ½å¤šæ¨¡æ€åˆ†æç³»ç»Ÿ')
st.caption("åŸºäº DeepSeek-V3 å¤§è¯­è¨€æ¨¡å‹çš„ç»¼åˆå¤„ç†å¹³å°")

# ä½¿ç”¨ Tabs åˆ†å‰²åŠŸèƒ½æ¨¡å— (ä¸°å¯ŒåŠŸèƒ½ç‚¹ï¼Œå‡‘ä»£ç é‡)
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ æ™ºèƒ½å¯¹è¯", "ğŸ“ æ–‡æœ¬åˆ†æå·¥å…·ç®±", "ğŸ“š æ–‡æ¡£çŸ¥è¯†åº“ (RAG)", "â„¹ï¸ å…³äºç³»ç»Ÿ"])

# === åŠŸèƒ½æ¨¡å— 1: æ™ºèƒ½å¯¹è¯ (å¤šè½®äº¤äº’) ===
with tab1:
    # 4.1 æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if msg["role"] == "user" else "ğŸ¤–"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])
            
    # 4.2 å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # ç”¨æˆ·æ¶ˆæ¯ä¸Šå±
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
            st.markdown(prompt)
            
        # æ„å»ºæ¶ˆæ¯ä¸Šä¸‹æ–‡ (åŒ…å« System Prompt + History)
        langchain_msgs = [SystemMessage(content=get_system_prompt(system_role))]
        # åªå–æœ€è¿‘ 10 æ¡å†å²ï¼Œé˜²æ­¢ token è¶…å‡º
        for m in st.session_state.messages[-10:]:
            if m["role"] == "user":
                langchain_msgs.append(HumanMessage(content=m["content"]))
            else:
                langchain_msgs.append(AIMessage(content=m["content"]))
        
        # AI å›å¤ç”Ÿæˆ
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            message_placeholder = st.empty()
            with st.spinner(f"[{system_role}] æ­£åœ¨æ€è€ƒä¸­..."):
                response_text = call_llm(langchain_msgs)
                
            if response_text:
                # æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ (è§†è§‰ä¼˜åŒ–)
                full_response = ""
                for chunk in response_text.split():
                    full_response += chunk + " "
                    time.sleep(0.02)
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
                
                # å­˜å…¥å†å²
                st.session_state.messages.append({"role": "assistant", "content": full_response})

# === åŠŸèƒ½æ¨¡å— 2: æ–‡æœ¬åˆ†æå·¥å…·ç®± (åˆ›æ–°ç‚¹ï¼šç‰¹å®šä»»åŠ¡å¤„ç†) ===
with tab2:
    st.header("NLP ç‰¹å®šä»»åŠ¡å¤„ç†")
    st.info("æ­¤æ¨¡å—ä¸ä¾èµ–ä¸Šä¸‹æ–‡ï¼Œç”¨äºå¤„ç†å•æ®µæ–‡æœ¬ä»»åŠ¡ã€‚")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        analysis_text = st.text_area("è¯·è¾“å…¥å¾…åˆ†æçš„æ–‡æœ¬:", height=300, placeholder="åœ¨æ­¤ç²˜è´´æ–‡ç« æˆ–æ®µè½...")
    
    with col2:
        st.subheader("é€‰æ‹©ä»»åŠ¡")
        task_type = st.radio("ä»»åŠ¡ç±»å‹", ["ğŸ“„ å†…å®¹æ‘˜è¦", "ğŸ‡¬ğŸ‡§ ä¸­è‹±äº’è¯‘", "ğŸ˜Š æƒ…æ„Ÿåˆ†æ", "ğŸ·ï¸ å…³é”®è¯æå–"])
        
        if st.button("å¼€å§‹åˆ†æ", type="primary"):
            if not analysis_text:
                st.warning("è¯·å…ˆè¾“å…¥æ–‡æœ¬ï¼")
            else:
                # æ ¹æ®ä»»åŠ¡æ„å»º Prompt
                prompt_templates = {
                    "ğŸ“„ å†…å®¹æ‘˜è¦": "è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œç®€æ˜æ‰¼è¦çš„æ‘˜è¦æ€»ç»“ï¼š\n\n",
                    "ğŸ‡¬ğŸ‡§ ä¸­è‹±äº’è¯‘": "è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼ˆå¦‚æœæ˜¯è‹±æ–‡åˆ™ç¿»è¯‘æˆä¸­æ–‡ï¼‰ï¼Œä¿æŒä¿¡è¾¾é›…ï¼š\n\n",
                    "ğŸ˜Š æƒ…æ„Ÿåˆ†æ": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰ï¼Œå¹¶è¯´æ˜ç†ç”±ï¼š\n\n",
                    "ğŸ·ï¸ å…³é”®è¯æå–": "è¯·æå–ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ Top 5 å…³é”®å®ä½“æˆ–æŠ€æœ¯æœ¯è¯­ï¼Œå¹¶ç”¨åˆ—è¡¨å½¢å¼å±•ç¤ºï¼š\n\n"
                }
                
                final_prompt = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„NLPæ–‡æœ¬åˆ†æå·¥å…·ã€‚"),
                    HumanMessage(content=prompt_templates[task_type] + analysis_text)
                ]
                
                with st.spinner("æ­£åœ¨æ‰§è¡Œ NLP ä»»åŠ¡..."):
                    result = call_llm(final_prompt)
                    if result:
                        st.success("åˆ†æå®Œæˆï¼")
                        st.markdown("### åˆ†æç»“æœ")
                        st.markdown(result)

# === åŠŸèƒ½æ¨¡å— 3: æ–‡æ¡£çŸ¥è¯†åº“ (RAG æ ¸å¿ƒåŠŸèƒ½) ===
with tab4:
    st.header("ğŸ“š æ–‡æ¡£é—®ç­” (RAG)")
    st.caption("ä¸Šä¼  PDF/TXT æ–‡æ¡£ï¼Œè®© AI åŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼ˆæ”¯æŒé•¿æ–‡æ¡£åˆ†æï¼‰")
    
    # 1. æ–‡ä»¶ä¸Šä¼ åŒº
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£ (æ”¯æŒ PDF/TXT)", type=["pdf", "txt"])
    
    if uploaded_file:
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        file_details = {"æ–‡ä»¶å": uploaded_file.name, "æ–‡ä»¶å¤§å°": f"{uploaded_file.size / 1024:.2f} KB"}
        st.success(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {uploaded_file.name}")
        
        # 2. æ–‡æ¡£è§£æ (æ•°æ®é¢„å¤„ç†)
        if "doc_content" not in st.session_state or st.session_state.current_file != uploaded_file.name:
            with st.spinner("æ­£åœ¨è§£ææ–‡æ¡£å†…å®¹..."):
                doc_text = extract_text_from_file(uploaded_file)
                if doc_text:
                    st.session_state.doc_content = doc_text
                    st.session_state.current_file = uploaded_file.name
                    st.info(f"æ–‡æ¡£è§£æå®Œæˆï¼Œå…±æå– {len(doc_text)} ä¸ªå­—ç¬¦ã€‚")
                else:
                    st.stop()
        
        # 3. æ–‡æ¡£é—®ç­”äº¤äº’
        st.markdown("---")
        rag_question = st.text_input("å…³äºè¿™ç¯‡æ–‡æ¡£ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ", placeholder="ä¾‹å¦‚ï¼šè¿™ç¯‡æ–‡ç« çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
        
        if st.button("ğŸ” åŸºäºæ–‡æ¡£æé—®", type="primary"):
            if not rag_question:
                st.warning("è¯·è¾“å…¥é—®é¢˜ï¼")
            elif not openai_api_key:
                st.warning("è¯·é…ç½® API Keyï¼")
            else:
                # 4. æ„å»º RAG Prompt (å…³é”®æŠ€æœ¯ï¼šContext Injection)
                # å°†æ–‡æ¡£å†…å®¹æ³¨å…¥åˆ° Prompt ä¸­ï¼Œåˆ©ç”¨ DeepSeek çš„é•¿çª—å£èƒ½åŠ›
                rag_prompt = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·ä»…æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸‹æ–‡èƒŒæ™¯ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ã€‚"),
                    HumanMessage(content=f"ã€èƒŒæ™¯æ–‡æ¡£å†…å®¹ã€‘ï¼š\n{st.session_state.doc_content}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{rag_question}")
                ]
                
                with st.spinner("AI æ­£åœ¨é˜…è¯»æ–‡æ¡£å¹¶ç”Ÿæˆç­”æ¡ˆ..."):
                    answer = call_llm(rag_prompt)
                    if answer:
                        st.markdown("### ğŸ¤– å›ç­”ç»“æœ")
                        st.markdown(answer)
                        
                        # åˆ›æ–°ç‚¹ï¼šå±•ç¤ºå¼•ç”¨æ¥æºï¼ˆæ¨¡æ‹Ÿï¼‰
                        with st.expander("æŸ¥çœ‹å‚è€ƒä¸Šä¸‹æ–‡"):
                            # ç®€å•å±•ç¤ºæ–‡æ¡£å‰500å­—ä½œä¸ºç¤ºæ„
                            st.text(st.session_state.doc_content[:1000] + "...")
    else:
        st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ ä¸€ä¸ªæ–‡æ¡£å¼€å§‹ä½“éªŒ")

# === åŠŸèƒ½æ¨¡å— 3: ç³»ç»Ÿè¯´æ˜ (æ–‡æ¡£å‡‘æ•°) ===
with tab3:
    st.markdown("### ç³»ç»Ÿæ¶æ„è¯´æ˜")
    st.markdown("""
    æœ¬ç³»ç»Ÿé‡‡ç”¨ **MVC (Model-View-Controller)** æ¶æ„è®¾è®¡ï¼š
    - **View (è§†å›¾å±‚)**: ä½¿ç”¨ `Streamlit` æ„å»º Web ç•Œé¢ï¼ŒåŒ…å«èŠå¤©çª—å£ã€ä¾§è¾¹æ å’Œå·¥å…·ç®±ã€‚
    - **Controller (æ§åˆ¶å±‚)**: è´Ÿè´£æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„å‚æ•°ï¼ˆTemperature, Roleï¼‰ï¼Œå¹¶è°ƒåº¦ API è°ƒç”¨ã€‚
    - **Model (æ¨¡å‹å±‚)**: åŸºäº `LangChain` æ¡†æ¶ï¼Œé›†æˆ `DeepSeek-V3` å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
    
    ### å…³é”®æŠ€æœ¯ç‚¹
    1. **Context Management**: ä½¿ç”¨ `Session State` ç®¡ç†å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ã€‚
    2. **Prompt Engineering**: é’ˆå¯¹ä¸åŒè§’è‰²ï¼ˆå­¦æœ¯ä¸“å®¶ã€ä»£ç å®¡è®¡å‘˜ï¼‰è®¾è®¡äº†å·®å¼‚åŒ–çš„ System Promptsã€‚
    3. **Error Handling**: å®Œæ•´çš„å¼‚å¸¸æ•è·æœºåˆ¶ï¼Œç¡®ä¿ API æ•…éšœæ—¶ç³»ç»Ÿä¸å´©æºƒã€‚
    """)
